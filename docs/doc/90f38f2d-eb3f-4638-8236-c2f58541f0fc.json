{
    "summary": "The code initializes a CLIP model class for encoding text into embeddings, using optional pre-trained models and tokenizers, with device specification. The encode method processes documents, stores embeddings, and encodes text efficiently, timing the process.",
    "details": [
        {
            "comment": "The code imports necessary libraries, defines a CLIPTextEncoder class that extends the Executor class from Jina, and initializes various parameters for the CLIP model. It takes optional arguments such as pretrained_model_name_or_path, base_tokenizer_model, max_length, device, traversal_paths, and batch_size. The class will be used to encode text into embeddings using the CLIP model.",
            "location": "\"/media/root/Prima/works/vced/docs/src/code/service/customClipText/clip_text.py\":0-29",
            "content": "from typing import Dict, Optional\nimport torch\nfrom docarray import DocumentArray\nfrom jina import Executor, requests\nimport clip\nimport time\nclass CLIPTextEncoder(Executor):\n    \"\"\"Encode text into embeddings using the CLIP model.\"\"\"\n    def __init__(\n        self,\n        pretrained_model_name_or_path: str = 'ViT-B/32',\n        base_tokenizer_model: Optional[str] = None,\n        max_length: int = 77,\n        device: str = 'cpu',\n        traversal_paths: str = '@r',\n        batch_size: int = 32,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        :param pretrained_model_name_or_path: Can be either:\n            - A string, the model id of a pretrained CLIP model hosted\n                inside a model repo on huggingface.co, e.g., 'openai/clip-vit-base-patch32'\n            - A path to a directory containing model weights saved, e.g., ./my_model_directory/\n        :param base_tokenizer_model: Base tokenizer model.\n            Defaults to ``pretrained_model_name_or_path`` if None\n        :param max_length: Max length argument for the tokenizer."
        },
        {
            "comment": "This code defines a class for initializing CLIP model with default parameters, such as max length, device type, and default traversal paths. It loads the pre-trained model and tokenizer using clip.load function and moves them to specified device.",
            "location": "\"/media/root/Prima/works/vced/docs/src/code/service/customClipText/clip_text.py\":30-52",
            "content": "            All CLIP models use 77 as the max length\n        :param device: Pytorch device to put the model on, e.g. 'cpu', 'cuda', 'cuda:1'\n        :param traversal_paths: Default traversal paths for encoding, used if\n            the traversal path is not passed as a parameter with the request.\n        :param batch_size: Default batch size for encoding, used if the\n            batch size is not passed as a parameter with the request.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.traversal_paths = traversal_paths\n        self.batch_size = batch_size\n        self.pretrained_model_name_or_path = pretrained_model_name_or_path\n        self.base_tokenizer_model = (\n            base_tokenizer_model or pretrained_model_name_or_path\n        )\n        self.max_length = max_length\n        self.device = device\n        model, preprocessor = clip.load(self.pretrained_model_name_or_path, device=device)\n        self.preprocessor = preprocessor\n        self.model = model\n        self.model.to(device)"
        },
        {
            "comment": "The code defines a class with an encode method that takes in a DocumentArray of documents, processes them by filtering for those with a non-empty text attribute and then batches them. It stores the embeddings in the embedding attribute. The tokenizer and model are instantiated from pretrained models but not shown here. The parameters dictionary can contain optional parameters to control encoding, namely traversal_paths and batch_size. If they're missing, default values are used.",
            "location": "\"/media/root/Prima/works/vced/docs/src/code/service/customClipText/clip_text.py\":54-76",
            "content": "        # self.tokenizer = CLIPTokenizer.from_pretrained(self.base_tokenizer_model)\n        # self.model = CLIPModel.from_pretrained(self.pretrained_model_name_or_path)\n        # self.model.eval().to(device)\n    @requests\n    def encode(self, docs: DocumentArray, parameters: Dict, **kwargs):\n        \"\"\"\n        Encode all documents with the `text` attribute and store the embeddings in the\n        `embedding` attribute.\n        :param docs: DocumentArray containing the Documents to be encoded\n        :param parameters: A dictionary that contains parameters to control encoding.\n            The accepted keys are ``traversal_paths`` and ``batch_size`` - in their\n            absence their corresponding default values are used.\n        \"\"\"\n        print('clip_text encode')\n        for docs_batch in DocumentArray(\n            filter(\n                lambda x: bool(x.text),\n                docs[parameters.get('traversal_paths', self.traversal_paths)],\n            )\n        ).batch(batch_size=parameters.get('batch_size', self.batch_size)) :"
        },
        {
            "comment": "The code encodes text using the `model.encode_text()` function, tokenizes it with `clip.tokenize()`, and assigns the resulting embeddings to each document in the batch. The time taken for this encoding process is printed as \"encode text cost\".",
            "location": "\"/media/root/Prima/works/vced/docs/src/code/service/customClipText/clip_text.py\":78-89",
            "content": "            text_batch = docs_batch.texts\n            t1 = time.time()\n            with torch.inference_mode():\n                input_tokens = [self.model.encode_text(clip.tokenize([t, \"unknown\"]).to(self.device)).cpu().to(dtype=torch.float32) for t in text_batch] # self._generate_input_tokens(text_batch)\n                embeddings = input_tokens  # self.model.get_text_features(**input_tokens).cpu().numpy()\n                for doc, embedding in zip(docs_batch, embeddings):\n                    doc.embedding = embedding\n                    # doc.embedding = np.array(embedding).astype('float32')[0]\n            t2 = time.time()\n            print(\"encode text cost:\", t2 - t1)\n            print(t1)\n            print(t2)"
        }
    ]
}