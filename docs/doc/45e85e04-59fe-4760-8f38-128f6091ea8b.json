{
    "summary": "The CLIPImageEncoder class encodes images into embeddings using the CLIP model with input parameters, initializes necessary modules, and defines an encode method for document batches. It loads the models onto the specified device, processes data, and assigns embeddings to instances.",
    "details": [
        {
            "comment": "The code defines a CLIPImageEncoder class, which is an Executor for encoding images into embeddings using the CLIP model. It takes input parameters such as pretrained_model_name_or_path (the CLIP model to use), device (CPU or GPU), batch_size (number of images to process at once), and traversal_paths (a string indicating how data should be accessed). This class initializes the necessary modules, including CLIPFeatureExtractor and CLIPModel.",
            "location": "\"/media/root/Prima/works/vced/docs/src/code/service/customClipImage/clip_image.py\":0-30",
            "content": "from typing import Optional, Tuple, Dict\nimport torch\nfrom docarray import DocumentArray\nfrom jina import Executor, requests\nfrom jina.logging.logger import JinaLogger\nfrom transformers import CLIPFeatureExtractor, CLIPModel\nimport numpy as np\nimport clip\nfrom PIL import Image\nimport time\nclass CLIPImageEncoder(Executor):\n    \"\"\"Encode image into embeddings using the CLIP model.\"\"\"\n    def __init__(\n        self,\n        pretrained_model_name_or_path: str = 'ViT-B/32',\n        device: str = 'cpu',\n        batch_size: int = 32,\n        traversal_paths: str = '@r',\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        :param pretrained_model_name_or_path: Can be either:\n            - A string, the model id of a pretrained CLIP model hosted\n                inside a model repo on huggingface.co, e.g., 'openai/clip-vit-base-patch32'\n            - A path to a directory containing model weights saved, e.g.,\n                ./my_model_directory/\n        :param base_feature_extractor: Base feature extractor for images."
        },
        {
            "comment": "This code defines a class with four parameters: `pretrained_model_name_or_path`, `use_default_preprocessing`, `device`, and `traversal_paths`. It also sets the default values for `batch_size` and initializes a logger. The superclass is initialized using `super().__init__(*args, **kwargs)`.",
            "location": "\"/media/root/Prima/works/vced/docs/src/code/service/customClipImage/clip_image.py\":31-49",
            "content": "            Defaults to ``pretrained_model_name_or_path`` if None\n        :param use_default_preprocessing: Whether to use the `base_feature_extractor` on\n            images (tensors) before encoding them. If you disable this, you must ensure\n            that the images you pass in have the correct format, see the ``encode``\n            method for details.\n        :param device: Pytorch device to put the model on, e.g. 'cpu', 'cuda', 'cuda:1'\n        :param traversal_paths: Default traversal paths for encoding, used if\n            the traversal path is not passed as a parameter with the request.\n        :param batch_size: Default batch size for encoding, used if the\n            batch size is not passed as a parameter with the request.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.batch_size = batch_size\n        self.traversal_paths = traversal_paths\n        self.pretrained_model_name_or_path = pretrained_model_name_or_path\n        self.logger = JinaLogger(self.__class__.__name__)\n        self.device = device"
        },
        {
            "comment": "This code initializes a CLIP model, preprocessor, and loads them onto the specified device. It then defines an encode method that takes in a DocumentArray, parameters, and processes document batches using a generator. The code also includes time tracking and prints information related to the input documents being processed.",
            "location": "\"/media/root/Prima/works/vced/docs/src/code/service/customClipImage/clip_image.py\":51-75",
            "content": "        model, preprocessor = clip.load(self.pretrained_model_name_or_path, device=device)\n        self.preprocessor = preprocessor\n        self.model = model\n        self.model.to(self.device)\n        # self.model.to(self.device).eval()\n    @requests\n    def encode(self, docs: DocumentArray, parameters: dict, **kwargs):\n        t1 = time.time()\n        print('clip_image encode', t1)\n        document_batches_generator =  DocumentArray(\n            filter(\n                lambda x: x is not None,\n                docs[parameters.get('traversal_paths', self.traversal_paths)],\n            )\n        ).batch(batch_size=parameters.get('batch_size', self.batch_size))\n        with torch.inference_mode():\n            for batch_docs in document_batches_generator:\n                print('in for')\n                for d in batch_docs:\n                    print('in clip image d.uri', d.uri, len(d.chunks))\n                    # tensor = self._generate_input_features(tensors_batch)\n                    tensors_batch = []\n                    for c in d.chunks:"
        },
        {
            "comment": "This code segment encodes an image into an embedding using a preprocessor, model, and specific device. It appends the resulting embeddings to a batch and assigns it as the embedding for a given data instance. The code includes unnecessary debug print statements and alternative encoding methods that are not implemented.",
            "location": "\"/media/root/Prima/works/vced/docs/src/code/service/customClipImage/clip_image.py\":76-89",
            "content": "                        if (c.modality == 'image'):\n                            image_embedding = self.model.encode_image(self.preprocessor(Image.fromarray(c.tensor)).unsqueeze(0).to(self.device))\n                            # tensors_batch.append(image_embedding)\n                            tensors_batch.append(np.array(image_embedding.cpu()).astype('float32'))\n                    embedding = tensors_batch\n                    # print(np.asarray(Image.open(d.uri)).shape)\n                    # image = self.preprocessor(Image.fromarray(np.asarray(Image.open(d.uri)))).unsqueeze(0).to(self.device)\n                    # embedding = self.model.encode_image(image)\n                    # print(embedding)\n                    # embedding = np.array(embedding).astype('float32')\n                    # print(embedding)\n                    d.embedding = embedding\n        t2 = time.time()\n        print('clip_image encode end', t2 - t1, t2)"
        }
    ]
}